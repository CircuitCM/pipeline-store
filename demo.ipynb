{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Usage\n",
    "\n",
    "Pipelines in this library use common notation, such as LangChain or R. However it differs from LangChain and other Pipeline libs through certain flexible differences."
   ],
   "id": "afc42bca22866a26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:18:36.408867Z",
     "start_time": "2025-11-19T16:18:36.399769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from collections.abc import Callable, Sequence\n",
    "\n",
    "#from pydantic import PrivateAttr\n",
    "\n",
    "from pipeline import Pipeline,Step,pipe_funcdict\n",
    "import asyncio as aio\n",
    "import time as tm\n",
    "\n",
    "def s1(n,a=None):\n",
    "    n.append(a)\n",
    "    print(n)\n",
    "    return n\n",
    "    \n",
    "class SimplePipe(Pipeline):\n",
    "    _functions: dict[str, tuple[Callable,tuple]] =pipe_funcdict(s1)\n",
    "\n",
    "print(SimplePipe._functions)\n",
    "    \n",
    "#Can be defined in various ways\n",
    "t1= SimplePipe.new('Simple 1') | Step(s1, a=1) | Step(s1, a=2) | Step(s1, a=3)\n",
    "t2=SimplePipe(name='Simple 2').step(s1,a=4).step(s1,a=5).step(s1,a=6)\n",
    "t3=SimplePipe(name='Simple 3',steps=[Step(s1,a=7),Step(s1,a=8),Step(s1,a=9)])\n",
    "\n",
    "print(repr(t1))\n",
    "await t1([]) #Note for now we still have to call the pipeline as an awaitable, as of current update a fully synchronous path still needs to be implemented.\n"
   ],
   "id": "90bba1229b6c7eb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default={'s1': (<function s1 at 0x000001D8FB58A160>, ('n', 'a'))}\n",
      "SimplePipe(name='Simple 1', steps=[Step(function='s1', args=[], kwargs={'a': 1}), Step(function='s1', args=[], kwargs={'a': 2}), Step(function='s1', args=[], kwargs={'a': 3})])\n",
      "[1]\n",
      "[1, 2]\n",
      "[1, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "While this looks like a generic Chaining/Pipeline library, we can see one major difference. Functions are registered in the KV store `_functions` of the Pipeline class definition. `Step` objects only hold this key and default arguments for the callable, making the execution graph independent of the functions it uses. You may suspect that this enables two especially modular features:\n",
    "1. The same steps/graph can be easily swapped into a different Pipeline definition. Maybe you are running a different environment where the implementation details are different, or perhaps you define classes for different library utilities or LLMs e.g. OpenAI modules vs Anthropic.\n",
    "2. `_functions` is a private attribute, Step keys are strings and default args are assumed to be serializable. As Pipeline and Step are pydantic models, the entire Pipeline object with included steps can be model serialized (see below). So Pipelines function as a network compatible, implementation agnostic procedure graph."
   ],
   "id": "5accc5746598c8c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Definition\n",
    "To define, first inherit the Pipeline class, then define `functions` using `pipe_funcdict` at the class level. Additionally, functions or coroutines must all have different `__qualname__` meaning that like function names imported from different modules will have conflicts. One remedy is to define static methods, or we can override the keys and then Step needs to be defined with a string:\n"
   ],
   "id": "eac4bbf41fed573a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T18:29:46.582520Z",
     "start_time": "2025-11-16T18:29:46.575065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class test1:\n",
    "    @staticmethod\n",
    "    def t(i): return i\n",
    "\n",
    "class test2:\n",
    "    @staticmethod\n",
    "    def t(i): return i\n",
    "    \n",
    "\n",
    "class Simple(Pipeline):\n",
    "    _functions = pipe_funcdict(test1.t, test2.t)\n",
    "\n",
    "print(Simple._functions)\n",
    "print('-')\n",
    "#or like\n",
    "t= lambda i: i\n",
    "t1 = lambda i:t(i)\n",
    "#t.__qualname__='t'\n",
    "#t1.__qualname__='t1'\n",
    "\n",
    "class Simple2(Pipeline):\n",
    "    _functions = pipe_funcdict(('t',t), ('t1',t1))\n",
    "    #if we don't override qualname then we need to override the name our Pipeline sees.\n",
    "\n",
    "print(Simple2._functions)\n",
    "print('-')\n",
    "\n",
    "#If we don't override qualname then our steps need to be created with our string reference instead of the incorrect qualname.\n",
    "Step('t'),Step(t) #see incorrect form\n"
   ],
   "id": "d4106cf80ff22433",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default={'test1.t': (<function test1.t at 0x000001754980A480>, ('i',)), 'test2.t': (<function test2.t at 0x000001754980A520>, ('i',))}\n",
      "-\n",
      "default={'t': (<function <lambda> at 0x000001754980A700>, ('i',)), 't1': (<function <lambda> at 0x000001754980A660>, ('i',))}\n",
      "-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Step(function='t', args=[], kwargs={}),\n",
       " Step(function='<lambda>', args=[], kwargs={}))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Runtime Mechanics\n",
    "Next instantiate a pipeline instance and build the procedure of your choice into it. The hierarchy is:\n",
    "1. Named variables (`kwargs`) in `Step` override named variables in the callables.\n",
    "2. Named variables with matching names in the `Pipeline` entry override in both the `Step` definition and the callable. This was chosen as there is otherwise no simple way to modify step variables for a single execution, using different named arguments in each function allows for per-call argument modification.\n",
    "3. Positional arguments in (`args`) in a step will be appended to existing args, args will overflow into kwargs when there are too many. This will override both first call named variables and step named variables.\n",
    "\n",
    "These behaviors and more are demonstrated here:"
   ],
   "id": "5203433cae15e0c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T15:02:08.314674Z",
     "start_time": "2025-11-16T15:02:05.108794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def test1(p,p1='p1',p2='p2',sleep=.5,_st=0.):\n",
    "    await aio.sleep(sleep)\n",
    "    et=tm.perf_counter()\n",
    "    df=et-p\n",
    "    print(f'Test 1 time offset: {df:.5f}, sleep: {sleep}, p1: {p1}, p2: {p2}, cumulative time: {et-_st:.5f}')\n",
    "    return et\n",
    "\n",
    "async def test2(p,p1='p1',p2='p2',sleep=.71,_st=0.):\n",
    "    await aio.sleep(sleep)\n",
    "    et=tm.perf_counter()\n",
    "    df=et-p\n",
    "    print(f'Test 2 time offset: {df:.5f}, sleep: {sleep}, p1: {p1}, p2: {p2}, cumulative time: {et-_st:.5f}')\n",
    "    return et#,df\n",
    "\n",
    "async def test3(p,kwt=None,p3='p1',p2='p2',sleep=.5,_st=0.):\n",
    "    await aio.sleep(sleep)\n",
    "    et=tm.perf_counter()\n",
    "    df=et-p\n",
    "    print(f'Test 3 time offset: {df:.5f}, sleep: {sleep}, p3: {p3}, p2: {p2}, cumulative time: {et-_st:.5f}, keyword test: {str(kwt)[:10]}')\n",
    "    return et\n",
    "\n",
    "def test4(p,p1='p1',p2='p2',sleep=.5,_st=0.):\n",
    "    tm.sleep(sleep)\n",
    "    et=tm.perf_counter()\n",
    "    df=et-p\n",
    "    print(f'Test 4 time offset: {df:.5f}, sleep: {sleep}, p1: {p1}, p2: {p2}, cumulative time: {et-_st:.5f}')\n",
    "    return et\n",
    "    \n",
    "\n",
    "class TestPipeline(Pipeline):\n",
    "    _functions: dict[str, tuple[Callable,tuple]] =pipe_funcdict(test1, test2, test3, test4)\n",
    "    \n",
    "\n",
    "testp1=TestPipeline(name='Test 1')|Step(test1,)|Step(test2,)|Step(test3,3,sleep=.4,kwt='sb int')|Step(test1,p1='2 last')|Step(test4)\n",
    "_st=tm.perf_counter()\n",
    "await aio.gather(testp1(_st,_st=_st),testp1(_st,p1='[not p3]',p2='[Alt time]',kwt='sb int',sleep=.6,_st=_st))"
   ],
   "id": "2b2e510a0990d2c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 time offset: 0.50116, sleep: 0.5, p1: p1, p2: p2, cumulative time: 0.50116\n",
      "Test 1 time offset: 0.60182, sleep: 0.6, p1: [not p3], p2: [Alt time], cumulative time: 0.60182\n",
      "Test 2 time offset: 0.60014, sleep: 0.6, p1: [not p3], p2: [Alt time], cumulative time: 1.20196\n",
      "Test 2 time offset: 0.70092, sleep: 0.71, p1: p1, p2: p2, cumulative time: 1.20208\n",
      "Test 3 time offset: 0.40101, sleep: 0.4, p3: p1, p2: p2, cumulative time: 1.60309, keyword test: 3\n",
      "Test 3 time offset: 0.61199, sleep: 0.6, p3: p1, p2: [Alt time], cumulative time: 1.81395, keyword test: 3\n",
      "Test 1 time offset: 0.49204, sleep: 0.5, p1: 2 last, p2: p2, cumulative time: 2.09512\n",
      "Test 4 time offset: 0.50075, sleep: 0.5, p1: p1, p2: p2, cumulative time: 2.59587\n",
      "Test 1 time offset: 0.78212, sleep: 0.6, p1: [not p3], p2: [Alt time], cumulative time: 2.59607\n",
      "Test 4 time offset: 0.60055, sleep: 0.6, p1: [not p3], p2: [Alt time], cumulative time: 3.19661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[642549.4583666, 642550.0591074]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For rule #1, see step definition test3 and the second to last step. Rule #2 is seen by the Alt time run. #3 is seen in test3, and additionally test4 demos sync function execution. Note that the second to last printout is delayed significantly longer than it's sleep time of 0.6, this is due to the first concurrent run forcefully freezing the event loop with synchronous sleep.  \n",
    "\n",
    "Unlike LangChain or LangGraph, pipelines can support both sync and async functions. Therefore running multiple Pipelines concurrently, or launching them as futures, can provide huge performance gains. This could support single threaded servers and benefit from packages like `uvloop` and `winloop`. But as seen in the example synchronous functions should have neglible completion latency, otherwise launching it in a separate thread.\n",
    "\n",
    "## Storing Pipelines\n",
    "\n",
    "As pipelines are pydantic models they can be serialized, stored, and loaded from a central repository for the class. No information about the used functions besides their name are stored, and they should still run even if you add or change step kwargs:"
   ],
   "id": "f8bf83045f947a99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T17:56:33.640303Z",
     "start_time": "2025-11-14T17:56:33.634303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Store\n",
    "d1=t1.model_dump_json(indent=2)\n",
    "d2=t2.model_dump_json(indent=2)\n",
    "d3=t3.model_dump_json(indent=2)\n",
    "print(d3)\n",
    "#This can now be written to a json file. Or\n",
    "#model_dump() and adding to a dict value, can then be drawn to one large json file.\n",
    "r3=SimplePipe.model_validate_json(d3)\n",
    "print(repr(r3))\n",
    "await r3([])\n"
   ],
   "id": "f50b175042618c31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Simple 3\",\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"function\": \"s1\",\n",
      "      \"args\": [],\n",
      "      \"kwargs\": {\n",
      "        \"a\": 7\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"function\": \"s1\",\n",
      "      \"args\": [],\n",
      "      \"kwargs\": {\n",
      "        \"a\": 8\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"function\": \"s1\",\n",
      "      \"args\": [],\n",
      "      \"kwargs\": {\n",
      "        \"a\": 9\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "SimplePipe(name='Simple 3', steps=[Step(function='s1', args=[], kwargs={'a': 7}), Step(function='s1', args=[], kwargs={'a': 8}), Step(function='s1', args=[], kwargs={'a': 9})])\n",
      "[7]\n",
      "[7, 8]\n",
      "[7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7, 8, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Potential applications include software, hardware, or domain specific procedure implementations. Safe procedural code generation. Network orchestration for tasks and recourses.",
   "id": "23c1ca854e9cee24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Advanced\n",
    "\n",
    "This demo replicates a DAG without needing a proper API. Pre-LangGraph when LangChain was still in early development, I used this method. It managed to be concise and graph-like. The method was to cache the output of certain callables over execution of the procedure that are expensive to regenerate or are non-deterministic. This implicitly creates fan-out dependency edges, fan-in is possible because all callables and awaitables are dispatched at the outer scope of step arguments. For example:"
   ],
   "id": "f1519147cd0f9a47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T22:11:33.981561Z",
     "start_time": "2025-11-14T22:11:32.486405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random as rng\n",
    "import math as mt\n",
    "from pipeline import Cache\n",
    "\n",
    "rcall=0\n",
    "\n",
    "graph=Cache()\n",
    "cache= graph.cache()\n",
    "\n",
    "@cache\n",
    "async def rand01(n=None):\n",
    "    global rcall\n",
    "    rcall+=1\n",
    "    rd=rng.random()\n",
    "    print('Random Call:',rcall)\n",
    "    #to demonstrate that the cache will save the future, and not the result, allowing an arbitrary # of dependencies to wait on it's completion\n",
    "    #and process as soon as possible after.\n",
    "    await aio.sleep(.5)\n",
    "    return rd\n",
    "\n",
    "def rpr(n,o1=None):\n",
    "    print('Print:', o1,f'{n:.5f}',f', Second Remainder: {-(mt.floor(et:=tm.perf_counter())-et):.5f}')\n",
    "    return n\n",
    "\n",
    "def pp(a,b):\n",
    "    return a + b\n",
    "    \n",
    "    \n",
    "class GraphDemo(Pipeline):\n",
    "    _functions: dict[str, tuple[Callable,tuple]] =pipe_funcdict(rand01, rpr, pp)\n",
    "    \n",
    "#Can be defined in various ways\n",
    "\n",
    "rs=Step(rand01)\n",
    "rg=GraphDemo(name='Random Gen')|rs #Call this node 1, our random recourse\n",
    "#graph.clear()\n",
    "\n",
    "#these will print at nearly the same time, and same random value.\n",
    "ot=await rg(n=None,o1=1)\n",
    "rpr(ot,1)\n",
    "ot=await rg(n=None,o1=2) #cache uses inputs, not inputs or default same as lru_cache\n",
    "rpr(ot,2)\n",
    "\n",
    "gpn=GraphDemo(name='Random and Print Node')|rs|Step(rpr)\n",
    "#we have more than a few ways to represent:\n",
    "ga1=GraphDemo(name='Add Node')|Step(pp)\n",
    "#ga1=GraphDemo(name='Graph Add 1')|Step(pp,a=gpn(),b=gpn())\n",
    "#ga1=GraphDemo(name='Graph Add 1')|Step(pp,a=gpn,b=gpn)\n",
    "ga1w=lambda r1=None, r2=None:ga1(a=gpn(n=r1,o1=1),b=gpn(n=r2,o1=2)) #the fan out print nodes are fan-in through the add. Four nodes total.\n",
    "ts=tm.perf_counter()\n",
    "await ga1w()\n",
    "print(f'Full time: {tm.perf_counter()-ts:.5f}') #demo that there is no wait\n",
    "graph.clear() #clear the \"graph nodes\" to try again.\n",
    "\n",
    "ts=tm.perf_counter()\n",
    "await ga1w()\n",
    "print(f'Full time: {tm.perf_counter()-ts:.5f}') #There is delay\n",
    "graph.clear() \n",
    "\n",
    "ts=tm.perf_counter()\n",
    "await ga1w(r1=1,r2=2)\n",
    "print(f'Full time: {tm.perf_counter()-ts:.5f}')\n",
    "#Now there are two implicitly random sources, 5 nodes total, but because they use concurrent recourses, the entire graph is still only 0.5 seconds delayed.\n"
   ],
   "id": "4d2d6464aaa3d923",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Call: 1\n",
      "Print: 1 0.60939 , Second Remainder: 0.21638\n",
      "Print: 2 0.60939 , Second Remainder: 0.21653\n",
      "Print: 1 0.60939 , Second Remainder: 0.21682\n",
      "Print: 2 0.60939 , Second Remainder: 0.21685\n",
      "Full time: 0.00021\n",
      "Random Call: 2\n",
      "Print: 1 0.71788 , Second Remainder: 0.71814\n",
      "Print: 2 0.71788 , Second Remainder: 0.71824\n",
      "Full time: 0.50144\n",
      "Random Call: 3\n",
      "Random Call: 4\n",
      "Print: 1 0.40423 , Second Remainder: 0.21820\n",
      "Print: 2 0.66231 , Second Remainder: 0.21829\n",
      "Full time: 0.49997\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We see the DAG is functional. It can also serialize Pipelines that are arguments of a Step class, though it may have it's limits.",
   "id": "731324eed3f81e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T22:13:01.277115Z",
     "start_time": "2025-11-14T22:13:01.270705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#To serialize and deserialize:\n",
    "ga1=GraphDemo(name='Graph Add 1')|Step(pp,a=GraphDemo().step(rand01,n=1).step(rpr),b=GraphDemo().step(rand01,n=2).step(rpr))\n",
    "gd=ga1.model_dump_json(indent=2)\n",
    "print(gd)\n",
    "import json #need to load in through json.loads, because lack of model_validate_json implementation at this point.\n",
    "gan=GraphDemo.model_validate(json.loads(gd))\n",
    "print(repr(gan))\n",
    "'plus',await gan()\n"
   ],
   "id": "43ac0fd7f1b2f2da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Graph Add 1\",\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"function\": \"pp\",\n",
      "      \"args\": [],\n",
      "      \"kwargs\": {\n",
      "        \"a\": {\n",
      "          \"name\": \"NA\",\n",
      "          \"steps\": [\n",
      "            {\n",
      "              \"function\": \"rand01\",\n",
      "              \"args\": [],\n",
      "              \"kwargs\": {\n",
      "                \"n\": 1\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"function\": \"rpr\",\n",
      "              \"args\": [],\n",
      "              \"kwargs\": {}\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        \"b\": {\n",
      "          \"name\": \"NA\",\n",
      "          \"steps\": [\n",
      "            {\n",
      "              \"function\": \"rand01\",\n",
      "              \"args\": [],\n",
      "              \"kwargs\": {\n",
      "                \"n\": 2\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"function\": \"rpr\",\n",
      "              \"args\": [],\n",
      "              \"kwargs\": {}\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "GraphDemo(name='Graph Add 1', steps=[Step(function='pp', args=[], kwargs={'a': GraphDemo(name='NA', steps=[Step(function='rand01', args=[], kwargs={'n': 1}), Step(function='rpr', args=[], kwargs={})]), 'b': GraphDemo(name='NA', steps=[Step(function='rand01', args=[], kwargs={'n': 2}), Step(function='rpr', args=[], kwargs={})])})])\n",
      "Print: None 0.40423 , Second Remainder: 0.51382\n",
      "Print: None 0.66231 , Second Remainder: 0.51386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('plus', 1.066538244471614)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:18:40.798833Z",
     "start_time": "2025-11-19T16:18:40.784350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#system style prompt, AGENTS.md, etc:\n",
    "c_style1= \"\"\"You are a math prodigy directly trained by Terence Tao.\"\"\"\n",
    "\n",
    "c_style2= \"\"\"You are a highly regarded physicist with decades long history of contributions.\"\"\"\n",
    "\n",
    "c_style3= \"\"\"You are the sum of all insights and talent that appeared on Curt Jaimungal's podcast.\"\"\"\n",
    "\n",
    "c_style4= \"\"\"You are Edward Witten.\"\"\"\n",
    "\n",
    "def fretrieval1(corpus): return f\"\"\"Comprehensively distill this corpus into research paths that have the most potential to prove Yang-Mills existence.\n",
    "\n",
    "Corpus:\n",
    "{corpus}\"\"\"\n",
    "\n",
    "def fquery1(ri):return f\"Prove the Yang-Mills existence theorem. Show all your work, even if you hit a dead end. You may utilize this research:\\n{ri}\"\n",
    "\n",
    "def fquery2(r1,r2,r3,ri):return f\"\"\"Utilize your colleagues research directions and basis of work to prove Yang-Mills existence. Go as far as you reasonably can.\n",
    "\n",
    "Colleague 1:\n",
    "{r1}\n",
    "\n",
    "Colleague 2:\n",
    "{r2}\n",
    "\n",
    "Colleague 3:\n",
    "{r3}\n",
    "\n",
    "Other research info:\n",
    "{ri}\"\"\"\n",
    "\n",
    "#DAG Procedure: Solve Yang-Mills.\n",
    "#Not, unlike as seen in the DAG every colleague call contains three nodes as a chain:\n",
    "# user prompt -> chat message -> query LLM\n",
    "\n",
    "#Colleague system prompts:\n",
    "c_style1,c_style2,c_style3,c_style4\n",
    "#f-strings:\n",
    "fretrieval1,fquery1,fquery2\n",
    "\n",
    "from pipeline import Pipeline, Cache\n",
    "#Cache used to replicate DAG\n",
    "cache=Cache()\n",
    "cwrap=cache.cache()\n",
    "#Log parameters\n",
    "llm_callct=0\n",
    "corpus_callct=0\n",
    "def reset_callct():\n",
    "    global llm_callct,corpus_callct\n",
    "    llm_callct=corpus_callct=0\n",
    "\n",
    "#Begin example.\n",
    "@cwrap\n",
    "def build_llmquery(user=\"\",system=\"\",_n=1):\n",
    "    \"\"\"When system, user, and _n are all the same as a previous call. `init_systemuser` will return the chat-like object that was stored in the cache.\n",
    "    \n",
    "    To achieve caching of the cllm_response, this function does need to be cached as cwrap will use the object id as fallback for objects without a __hash__, like lists.\n",
    "    \"\"\"\n",
    "    return [{\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},]\n",
    "\n",
    "\n",
    "async def llm_response(chat, model: str = \"gpt\") -> str: \n",
    "    \"\"\"An over simplified LLM response that sends back the text only.\"\"\"\n",
    "    global llm_callct\n",
    "    llm_callct+=1\n",
    "    return 'Response to YM solution!'\n",
    "\n",
    "cllm_response=cwrap(llm_response) #cached version\n",
    "cllm_response.__qualname__='cllm_response' #needed for GraphLLM to see the separate definition. or we can manually override.\n",
    "\n",
    "@cwrap #can cache this if it's expensive, however it's not necessary to achieve caching of llm_response, cwrap is to demo total # of calls\n",
    "def load_corpus(resource_path: str = \"\") -> str: \n",
    "    global corpus_callct\n",
    "    corpus_callct+=1\n",
    "    return \"Research on YM existence!\"\n",
    "\n",
    "\n",
    "class GraphLLM(Pipeline):\n",
    "    _functions: dict[str, tuple[Callable,tuple]] =pipe_funcdict(load_corpus, build_llmquery, cllm_response, llm_response, fretrieval1, fquery1, fquery2)\n",
    "  \n",
    "make_rinfo=(GraphLLM(name='Make Research Info')\n",
    "            |Step(load_corpus,recourse_path='Some Path.')\n",
    "            |Step(fretrieval1) #f-strings have their own internal cache and hash if too large to be cached.\n",
    "            |Step(build_llmquery, system=c_style3)\n",
    "            |Step(cllm_response)\n",
    "            ) #Corpus node.\n",
    "mk_colleague=lambda s_style:(\n",
    "        make_rinfo.pipe_copy(name='First Solve')\n",
    "        |Step(fquery1)\n",
    "        |Step(build_llmquery,system=s_style)\n",
    "        |Step(llm_response)\n",
    "        ) #Colleague node and corpus edge.\n",
    "#Prove YM node and edges.\n",
    "solve_c1,solve_c2,solve_c3=mk_colleague(c_style1),mk_colleague(c_style2),mk_colleague(c_style3)\n",
    "\n",
    "proveym_graph=(GraphLLM(name='Prove Yang Mills')\n",
    "               |Step(fquery2,r1=solve_c1,r2=solve_c2,r3=solve_c3,ri=make_rinfo)#or,solve_c1,solve_c2,solve_c3,make_rinfo) #or nothing\n",
    "               |Step(build_llmquery, system=c_style4)\n",
    "               |Step(llm_response)\n",
    "               )\n",
    "\n",
    "await proveym_graph()\n",
    "print(f'Corpus Calls: {corpus_callct}') #Should be 1\n",
    "print(f'LLM Calls: {llm_callct}') #Should be 5\n",
    "print('-')\n",
    "reset_callct()\n",
    "#Now because the recourse path hasn't changed, subtract 1 function calls.\n",
    "await proveym_graph()\n",
    "print(f'Corpus Calls: {corpus_callct}') #Should be 0\n",
    "print(f'LLM Calls: {llm_callct}') #Should be 4\n",
    "print('-')\n",
    "reset_callct()\n",
    "cache.clear()\n",
    "#We cleared the cache so return to full # of graph calls.\n",
    "await proveym_graph()\n",
    "print(f'Corpus Calls: {corpus_callct}')\n",
    "print(f'LLM Calls: {llm_callct}')\n",
    "print('-')\n",
    "#Last if we commented out #@cwrap we'd get:\n",
    "#Corpus Calls: 4\n",
    "#LLM Calls: 8\n",
    "\n",
    "#also await proveym_graph(solve_c1,solve_c2,solve_c3,make_rinfo,system=s_style4) #works\n",
    "#As a production template we can implement load_corpus to take a recourse path from e.g. a task queue.\n",
    "#We then have an endpoint function that routes to or generates the correct pipeline, and launches it as a future\n",
    "#where the last step is a POST to the correct address, or whichever needed result.\n",
    "#resource, recourse \n",
    "#If necessary we can also alter the initial path from top-scope like so:\n",
    "dp='Different Path.'\n",
    "mkc3=lambda recourse_path:(\n",
    "    mk_colleague(s)(resource_path=recourse_path) for s in (c_style1, c_style2, c_style3))\n",
    "#this will produce awaitables so mkc3 has to be called like this at each graph entry.\n",
    "await proveym_graph(*mkc3(dp), make_rinfo(recourse_path=dp), system=c_style4)"
   ],
   "id": "60c4f47bcd7dfffa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Calls: 1\n",
      "LLM Calls: 5\n",
      "-\n",
      "Corpus Calls: 0\n",
      "LLM Calls: 4\n",
      "-\n",
      "Corpus Calls: 1\n",
      "LLM Calls: 5\n",
      "-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Response to YM solution!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cb1ef164fe3dd147"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
